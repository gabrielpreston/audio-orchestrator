# discord-voice-lab environment template
#
# This sample shows the new split between shared defaults and service-specific
# configuration files. Copy each section into the matching path before running
# `make dev-*` or `make run`.

########################
# ./.env.common       #
########################
LOG_LEVEL=info
LOG_JSON=true

# Logging sampling and rate limiting
LOG_SAMPLE_VAD_N=50
LOG_SAMPLE_UNKNOWN_USER_N=100
LOG_RATE_LIMIT_PACKET_WARN_S=10

# OpenTelemetry Configuration (OBSERVABILITY STACK REMOVED - Commented out)
# To re-enable observability: uncomment the following lines and set OTEL_ENABLED=true
# Note: Requires observability stack (otel-collector, prometheus, jaeger, grafana) to be running
# OTEL_ENABLED=true
# # Use HTTP protocol on port 4318 (gRPC is on 4317)
# OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4318
# OTEL_EXPORTER_OTLP_PROTOCOL=http
# OTEL_TRACES_SAMPLER=parentbased_traceratio
# OTEL_TRACES_SAMPLER_ARG=1.0

# Service URL Configuration (Standardized {SERVICE}_BASE_URL pattern)
# All service-to-service communication URLs (agnostic service names)
# These are shared across all services via .env.common
AUDIO_BASE_URL=http://audio:9100
STT_BASE_URL=http://stt:9000
ORCHESTRATOR_BASE_URL=http://orchestrator:8200
LLM_BASE_URL=http://flan:8100
TTS_BASE_URL=http://bark:7100
GUARDRAILS_BASE_URL=http://guardrails:9300
DISCORD_BASE_URL=http://discord:8001

#######################################
# ./services/discord/.env.service     #
#######################################
DISCORD_BOT_TOKEN=changeme
DISCORD_GUILD_ID=000000000000000000
DISCORD_VOICE_CHANNEL_ID=000000000000000000
DISCORD_AUTO_JOIN=false
DISCORD_INTENTS=guilds,guild_voice_states
DISCORD_VOICE_CONNECT_TIMEOUT=15
DISCORD_VOICE_CONNECT_ATTEMPTS=3
DISCORD_VOICE_RECONNECT_BASE_DELAY=5
DISCORD_VOICE_RECONNECT_MAX_DELAY=60
AUDIO_ALLOWLIST=
AUDIO_SILENCE_TIMEOUT=0.75
AUDIO_MAX_SEGMENT_DURATION=15
AUDIO_MIN_SEGMENT_DURATION=0.3
AUDIO_AGGREGATION_WINDOW=1.5
AUDIO_SAMPLE_RATE=48000
AUDIO_VAD_SAMPLE_RATE=16000
AUDIO_VAD_FRAME_MS=30
AUDIO_VAD_AGGRESSIVENESS=1
STT_TIMEOUT=45
STT_MAX_RETRIES=3
STT_FORCED_LANGUAGE=en
STT_VAD_FILTER=false
WAKE_MODEL_PATHS=
WAKE_PHRASES=hey atlas,ok atlas,atlas,hey assistant,ok assistant
ORCHESTRATOR_WAKE_PHRASES=
WAKE_THRESHOLD=0.3
WAKE_SAMPLE_RATE=16000
METRICS_PORT=
WAVEFORM_DEBUG_DIR=

# Unified Resilience Pattern Configuration
# All service-to-service HTTP calls now use ResilientHTTPClient with circuit breaker protection.
# These variables control circuit breaker behavior, timeouts, health checks, and connection pooling.
# Prefix patterns: ORCHESTRATOR_*, AUDIO_PROCESSOR_*, STT_AUDIO_PROCESSOR_*, GUARDRAILS_*
#
# Circuit Breaker Configuration
# ORCHESTRATOR_CIRCUIT_FAILURE_THRESHOLD=5        # Failures before opening circuit
# ORCHESTRATOR_CIRCUIT_SUCCESS_THRESHOLD=2        # Successes to close from half-open
# ORCHESTRATOR_CIRCUIT_TIMEOUT_SECONDS=30.0      # Base timeout for circuit recovery
#
# HTTP Client Configuration
# ORCHESTRATOR_TIMEOUT_SECONDS=30.0               # Request timeout in seconds
# ORCHESTRATOR_HEALTH_CHECK_INTERVAL=10.0        # Seconds between health checks
# ORCHESTRATOR_HEALTH_CHECK_STARTUP_GRACE_SECONDS=30.0  # Grace period during startup
#
# Connection Pooling Configuration
# ORCHESTRATOR_MAX_CONNECTIONS=10                 # Max concurrent connections
# ORCHESTRATOR_MAX_KEEPALIVE_CONNECTIONS=5       # Max persistent connections

###################################
# ./services/stt/.env.service     #
###################################
FW_MODEL=medium.en
FW_DEVICE=cuda
FW_COMPUTE_TYPE=float16
FW_ENABLE_ENHANCEMENT=false

# Model download and cache configuration
FORCE_MODEL_DOWNLOAD_WHISPER_MODEL=false

# STT performance optimizations
# Note: torch.compile() is NOT applicable - faster-whisper uses CTranslate2 backend
# Pre-warming to ensure models are ready before serving traffic
STT_ENABLE_PREWARM=true

# Result caching for identical audio requests (significant speedup for repeated audio)
STT_ENABLE_CACHE=true
STT_CACHE_MAX_ENTRIES=200  # Maximum number of cached transcripts
STT_CACHE_MAX_SIZE_MB=1000  # Maximum cache size in megabytes

#######################################
# ./services/guardrails/.env.service #
#######################################
PORT=9300
TOXICITY_MODEL=unitary/toxic-bert
ENABLE_PII_DETECTION=true
MAX_CONCURRENT_REQUESTS=10
REQUEST_TIMEOUT_MS=5000
CACHE_SIZE=1000
CACHE_TTL_SECONDS=3600

# Model download and cache configuration
FORCE_MODEL_DOWNLOAD_TOXICITY_MODEL=false
HF_HOME=/app/models

#######################################
# ./services/flan/.env.service     #
#######################################
PORT=8100
FLAN_T5_MODEL_SIZE=google/flan-t5-large
TRANSFORMERS_CACHE=/app/models
ENABLE_MODEL_CACHING=true
MAX_SEQUENCE_LENGTH=512
TEMPERATURE=0.7
TOP_P=0.9
TOP_K=50
REPETITION_PENALTY=1.1

# Model download and cache configuration
HF_HOME=/app/models
FORCE_MODEL_DOWNLOAD_FLAN_T5=false

# FLAN performance optimizations
# torch.compile() optimization (20-40% speedup on PyTorch 2.0+)
FLAN_ENABLE_TORCH_COMPILE=true
FLAN_COMPILE_MODE=default  # Options: default, reduce-overhead, max-autotune, max-autotune-no-cudagraphs

# Pre-warming to trigger torch.compile() warmup during startup
FLAN_ENABLE_PREWARM=true

# Result caching for repeated prompts (optional, disabled by default)
# Less useful than STT/Bark caching since prompts are typically unique
FLAN_ENABLE_CACHE=false
FLAN_CACHE_MAX_ENTRIES=100  # Maximum number of cached generations
FLAN_CACHE_MAX_SIZE_MB=500  # Maximum cache size in megabytes

#######################################
# ./services/orchestrator/.env.service #
#######################################
PORT=8200
LANGCHAIN_VERBOSE=true
MAX_TOKENS=512
TEMPERATURE=0.7
ENABLE_TOOL_CALLING=true
ENABLE_GUARDRAILS=true

########################
# ./.env.common       #
########################
LOG_LEVEL=info
LOG_JSON=true

# Logging sampling and rate limiting
# Sample high-frequency events to reduce log volume in production.
LOG_SAMPLE_VAD_N=50
LOG_SAMPLE_UNKNOWN_USER_N=100
LOG_RATE_LIMIT_PACKET_WARN_S=10

# OpenTelemetry Configuration (OBSERVABILITY STACK REMOVED - Commented out)
# To re-enable observability: uncomment the following lines and set OTEL_ENABLED=true
# Note: Requires observability stack (otel-collector, prometheus, jaeger, grafana) to be running
# OTEL_ENABLED=true
# # Use HTTP protocol on port 4318 (gRPC is on 4317)
# OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4318
# OTEL_EXPORTER_OTLP_PROTOCOL=http
# OTEL_TRACES_SAMPLER=parentbased_traceratio
# OTEL_TRACES_SAMPLER_ARG=1.0

########################
# ./.env.docker       #
########################
PUID=1000
PGID=1000
TZ=Etc/UTC

#######################################
# ./services/bark/.env.service     #
#######################################
PORT=7100
BARK_USE_SMALL_MODELS=true  # Enable for 30-50% speedup (slight quality trade-off)
DEFAULT_VOICE=v2/en_speaker_1
MAX_TEXT_LENGTH=1000
SAMPLE_RATE=22050

# Bark performance optimizations
# torch.compile() optimization (replaces deprecated Better Transformer)
BARK_ENABLE_TORCH_COMPILE=true
BARK_COMPILE_MODE=max-autotune-no-cudagraphs  # Options: default, reduce-overhead, max-autotune, max-autotune-no-cudagraphs

# Pre-warming to trigger torch.compile() warmup during startup
BARK_ENABLE_PREWARM=true

# Result caching for repeated synthesis requests
BARK_ENABLE_CACHE=true
BARK_CACHE_MAX_ENTRIES=100  # Maximum number of cached results
BARK_CACHE_MAX_SIZE_MB=500  # Maximum cache size in megabytes

# INT8 quantization (disabled by default, requires quality validation)
# Note: INT8 and FP16 are mutually exclusive - INT8 requires FP32 models
BARK_ENABLE_INT8_QUANTIZATION=false

# Model download and cache configuration
FORCE_MODEL_DOWNLOAD_BARK_MODELS=false

# Hugging Face and cache directory configuration
HF_HOME=/app/models
HOME=/app
XDG_CACHE_HOME=/app/models

# PyTorch threading optimization (match CPU count)
OMP_NUM_THREADS=8
MKL_NUM_THREADS=8

# CUDA memory optimization (enhances base image setting)
# Base image sets: PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128
# This adds expandable_segments if supported (verify PyTorch version compatibility)
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True

#######################################
# ./services/audio/.env.service #
#######################################
PORT=9100
ENABLE_ENHANCEMENT=true
ENABLE_VAD=true
ENABLE_HIGH_PASS=true
ENABLE_VOLUME_NORMALIZATION=true
ENABLE_NOISE_REDUCTION=true
VAD_AGGRESSIVENESS=1
HIGH_PASS_CUTOFF=80.0
TARGET_RMS=0.1
MAX_CONCURRENT_REQUESTS=10
FRAME_PROCESSING_TIMEOUT_MS=20
ENHANCEMENT_TIMEOUT_MS=50
METRICGAN_MODEL_SOURCE=speechbrain/metricgan-plus-voicebank
METRICGAN_MODEL_SAVEDIR=/app/models/metricgan-plus

# Model download configuration
FORCE_MODEL_DOWNLOAD_METRICGAN=false
PORT=7000
TTS_MODEL_PATH=/app/models/piper/en_US-amy-medium.onnx
TTS_MODEL_CONFIG_PATH=/app/models/piper/en_US-amy-medium.onnx.json
TTS_DEFAULT_VOICE=
TTS_MAX_TEXT_LENGTH=1000
TTS_MAX_CONCURRENCY=4
TTS_RATE_LIMIT_PER_MINUTE=60
TTS_AUTH_TOKEN=changeme
TTS_LENGTH_SCALE=1.0
TTS_NOISE_SCALE=0.667
TTS_NOISE_W=0.8

# Discord warm-up and logging sampling
DISCORD_WARMUP_AUDIO=true
LOG_SAMPLE_SEGMENT_READY_RATE=
LOG_SAMPLE_SEGMENT_READY_N=

# STT warm-up
STT_WARMUP=true

# Discord service modes
DISCORD_FULL_BOT=false
DISCORD_HTTP_MODE=false
DISCORD_MCP_MODE=false

# Orchestrator client timeout (used by Discord client)
ORCH_TIMEOUT=30

###################################
# Force Model Download (optional) #
###################################
# Set to true to force re-download of models on startup (bypasses cache)
# Global setting applies to all services unless service-specific override is set
# FORCE_MODEL_DOWNLOAD=false

# Service-specific overrides (override global setting)
# FORCE_MODEL_DOWNLOAD_WHISPER_MODEL=false
# FORCE_MODEL_DOWNLOAD_FLAN_T5=false
# FORCE_MODEL_DOWNLOAD_TOXICITY_MODEL=false
# FORCE_MODEL_DOWNLOAD_BARK_MODELS=false
# FORCE_MODEL_DOWNLOAD_METRICGAN=false

# LLM generation defaults (overridable per-request)
LLM_MAX_TOKENS=128
LLM_TEMPERATURE=0.7
LLM_TOP_P=0.9
LLM_TOP_K=40
LLM_REPEAT_PENALTY=1.1

#######################################
# Integration Test Configuration      #
#######################################
# All integration tests use standardized {SERVICE}_BASE_URL pattern with agnostic service names.
# These variables can be overridden for different test environments (local, CI, etc.)
#
# Service URL Environment Variables (agnostic service names):
# AUDIO_BASE_URL=http://audio:9100
# STT_BASE_URL=http://stt:9000
# ORCHESTRATOR_BASE_URL=http://orchestrator:8200
# LLM_BASE_URL=http://flan:8100          # Service: LLM, implementation: FLAN-T5
# TTS_BASE_URL=http://bark:7100          # Service: TTS, implementation: Bark
# GUARDRAILS_BASE_URL=http://guardrails:9300
# DISCORD_BASE_URL=http://discord:8001
# TESTING_BASE_URL=http://testing:8080
#
# Example: Override for local testing
# export LLM_BASE_URL=http://localhost:8110
# export TTS_BASE_URL=http://localhost:7120
# pytest services/tests/integration/
