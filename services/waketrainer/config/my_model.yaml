# Example Wake Word Training Configuration
# Based on openWakeWord training config structure
# See: https://github.com/dscripka/openWakeWord

# Model name (directory / filename prefix)
model_name: "hey_ao"

# Target wake phrase variants for A-O / “Hey-oh!”
# Multiple phrases still train a single binary detector.
target_phrase:
  - "hey oh"
  - "hey-oh"
  - "heyo"
  - "hey o"
  - "hey a o"
  - "hey ao"
  - "a o"
  - "ay oh"
  - "ayo"
  - "ay-o"
  - "a oh"
  - "hey-ohh"
  - "heeeey oh"
  - "ayooo"

# Hard negatives to reduce common near-phonetic false triggers
custom_negative_phrases:
  - "hello"
  - "halo"
  - "haloed"
  - "hey ho"
  - "hey all"
  - "hey y'all"
  - "hey owl"
  - "hey owen"
  - "hey oh no"
  - "a hole"
  - "a home"
  - "a oath"
  - "a hope"
  - "ayo bro"
  - "ay old"
  - "hey old"
  - "hey hold"
  - "ok google"
  - "hey google"
  - "hey alexa"
  - "hey siri"
  - "ok atlas"
  - "hey atlas"
  - "hey audio"
  - "aol"
  - "a ok"
  - "a okay"
  - "aye oh"
  - "i owe"
  - "ayo tech"
  - "ayo dude"

# Positive sample counts (TTS + augment)
# Reduced for testing/validation - sufficient for model testing while fitting in GPU memory
n_samples: 25000  # Reduced from 120000 - minimum recommended is 20000, 25000 is sufficient for testing
n_samples_val: 200  # Increased from 25 to 200 for better validation (still small enough to avoid OOM)

# TTS / augmentation settings
# Reduced batch sizes to prevent CUDA OOM during generation
# Smaller batches = more batches but less GPU memory per batch
tts_batch_size: 10  # Reduced from 20 to 10 - prevents OOM during TTS generation (25k samples = 2500 batches)
augmentation_batch_size: 16  # Keep at 16 for variety in augmentation

# Path to Piper TTS sample generator
piper_sample_generator_path: "/workspace/piper-sample-generator"

# Output directory for generated data, features, and trained models
output_dir: "/workspace/services/models/wake/detection/hey_ao"

# Room Impulse Responses (RIRs) for realistic reverb
# Point to the 16khz subdirectory containing WAV files directly (not the parent directory)
rir_paths:
  - "/workspace/services/models/wake/training-data/mit_rirs/16khz"

# Background audio directories
# WAV files extracted from Parquet dataset using: make extract-background-audio
background_paths:
  - "/workspace/services/models/wake/training-data/background_clips/wav"

# Duplication rate for background audio (>=1)
background_paths_duplication_rate:
  - 1

# Precomputed features for false-positive validation
# Using a smaller subset (10,000 samples) to avoid OOM while still providing validation
# The full validation set (481,345 samples) causes OOM due to memory-intensive reshape operations
# To use the full dataset, change path to: /workspace/services/models/wake/training-data/validation_set_features.npy
# To create a custom subset: make wake-train-create-validation-subset SIZE=10000
false_positive_validation_data_path: "/workspace/services/models/wake/training-data/validation_set_features_small.npy"

# Number of augmentation passes per generated clip
augmentation_rounds: 1

# Precomputed feature files for large negative dataset(s)
feature_data_files:
  "ACAV100M_sample": "/workspace/services/models/wake/training-data/openwakeword_features_ACAV100M_2000_hrs_16bit.npy"

# Batch composition (sum == total batch size)
# Aggressively reduced batch sizes to fit within 8GB Docker memory limit and GPU memory
# Original: 1024 + 50 + 60 = 1134 (caused OOM at 75% training)
# First reduction: 512 + 25 + 30 = 567 (caused CUDA OOM at 79% training)
# Second reduction: 256 + 12 + 15 = 283 (still CUDA OOM at 79% during validation)
# Current: 128 + 6 + 8 = 142 (approximately 87% reduction from original)
batch_n_per_class:
  "ACAV100M_sample": 128
  "adversarial_negative": 6
  "positive": 8

# Model architecture
model_type: "dnn"
layer_size: 32

# Training parameters
# Reduced for testing - 15000 steps is sufficient for validation/testing purposes
# Validation will start at 75% (11250 steps) with early stopping to prevent overfitting
steps: 15000
max_negative_weight: 1500
target_false_positives_per_hour: 0.2

# Optimizer settings
learning_rate: 0.001
optimizer: "adam"

# Early stopping
early_stopping_patience: 5000
early_stopping_min_delta: 0.001
