# FLAN-T5 LLM Service Configuration

# Model configuration (easy to swap)
FLAN_T5_MODEL_SIZE=google/flan-t5-large
# Alternative options:
# FLAN_T5_MODEL_SIZE=google/flan-t5-base  # 1GB RAM
# FLAN_T5_MODEL_SIZE=google/flan-t5-xl    # 16GB RAM

# Generation parameters
FLAN_T5_MAX_LENGTH=512
FLAN_T5_TEMPERATURE=0.7
FLAN_T5_TOP_P=0.9
PORT=8100

# Model download and cache configuration
HF_HOME=/app/models
FORCE_MODEL_DOWNLOAD_FLAN_T5=false

# FLAN performance optimizations
# torch.compile() optimization (20-40% speedup on PyTorch 2.0+)
FLAN_ENABLE_TORCH_COMPILE=true
FLAN_COMPILE_MODE=default

# Pre-warming to trigger torch.compile() warmup during startup
FLAN_ENABLE_PREWARM=true

# Result caching for repeated prompts (optional, disabled by default)
FLAN_ENABLE_CACHE=false
FLAN_CACHE_MAX_ENTRIES=100
FLAN_CACHE_MAX_SIZE_MB=500
