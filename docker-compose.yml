version: "3.8"
services:
  # Enhanced AI Services
  guardrails:
    build:
      context: "."
      dockerfile: "services/guardrails/Dockerfile"
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/guardrails:latest"
    ports:
      - "9310:9300" # External: 9310, Internal: 9300
    env_file:
      - "./.env.common"
      - "./.env.docker"
      - "./services/guardrails/.env.service"
    environment:
      - TOXICITY_MODEL=unitary/toxic-bert
      - ENABLE_PII_DETECTION=true
    healthcheck:
      test:
        [
          "CMD",
          "python",
          "/app/scripts/health_check.py",
          "http://localhost:9300/health/ready",
          "--timeout",
          "5",
        ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 2G # Toxicity detection
          cpus: "1"
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  orchestrator-enhanced:
    build:
      context: "."
      dockerfile: "services/orchestrator_enhanced/Dockerfile"
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/orchestrator-enhanced:latest"
    ports:
      - "8220:8200" # External: 8220, Internal: 8200
    env_file:
      - "./.env.common"
      - "./.env.docker"
      - "./services/orchestrator_enhanced/.env.service"
    environment:
      - LLM_PRIMARY_URL=http://llm-flan:8100
      - GUARDRAILS_URL=http://guardrails:9300
    depends_on:
      llm-flan:
        condition: service_healthy
      audio-processor:
        condition: service_healthy
      stt:
        condition: service_healthy
    healthcheck:
      test:
        [
          "CMD",
          "python",
          "/app/scripts/health_check.py",
          "http://localhost:8200/health/ready",
          "--timeout",
          "5",
        ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 2G # LangChain + overhead
          cpus: "2"
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  llm-flan:
    build:
      context: "."
      dockerfile: "services/llm_flan/Dockerfile"
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/llm-flan:latest"
    ports:
      - "8110:8100" # External: 8110, Internal: 8100
    env_file:
      - "./.env.common"
      - "./.env.docker"
      - "./services/llm_flan/.env.service"
    environment:
      - MODEL_NAME=${FLAN_T5_MODEL_SIZE:-google/flan-t5-large}
      - TRANSFORMERS_CACHE=/app/models
    volumes:
      - ./services/models/flan-t5:/app/models:ro
      - ~/.cache/huggingface:/root/.cache/huggingface:ro
    # No dependencies - FLAN-T5 is standalone
    healthcheck:
      test:
        [
          "CMD",
          "python",
          "/app/scripts/health_check.py",
          "http://localhost:8100/health/ready",
          "--timeout",
          "10",
        ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 300s
    deploy:
      resources:
        limits:
          memory: 12G # Increased for FLAN-T5 Large + overhead
          cpus: "4"
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"
  audio-processor:
    build:
      context: "."
      dockerfile: "services/audio_processor/Dockerfile"
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/audio-processor:latest"
    ports:
      - "8010:9100"
    env_file:
      - "./.env.common"
      - "./.env.docker"
      - "./services/audio_processor/.env.service"
    volumes:
      - "./debug:/app/debug"
      - "./services/audio_processor/config:/app/config"
    healthcheck:
      test: >
        ["CMD", "curl", "-f", "http://localhost:9100/health/ready"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  discord:
    build:
      context: "."
      dockerfile: "services/discord/Dockerfile"
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/discord:latest"
    ports:
      - "8009:8001"
    env_file:
      - "./.env.common"
      - "./.env.docker"
      - "./services/discord/.env.service"
    volumes:
      - "./debug:/app/debug"
    healthcheck:
      test: >
        ["CMD", "python", "/app/scripts/health_check.py",
         "http://localhost:8001/health/ready", "--timeout", "5"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 45s
    depends_on:
      audio-processor:
        condition: service_healthy
      stt:
        condition: service_healthy
      orchestrator:
        condition: service_healthy
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  stt:
    build:
      context: "."
      dockerfile: "services/stt/Dockerfile"
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/stt:latest"
    ports:
      - "8011:9000"
    env_file:
      - "./.env.common"
      - "./.env.docker"
      - "./services/stt/.env.service"
    volumes:
      - "./services/models/stt:/app/models:ro"
      - "./debug:/app/debug"
    healthcheck:
      test: >
        ["CMD", "python", "/app/scripts/health_check.py",
         "http://localhost:9000/health/ready", "--timeout", "5"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    depends_on:
      audio-processor:
        condition: service_healthy
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  # Bark TTS Service
  tts-bark:
    build:
      context: "."
      dockerfile: "services/tts_bark/Dockerfile"
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/tts-bark:latest"
    ports:
      - "7120:7100" # External: 7120, Internal: 7100
    env_file:
      - "./.env.common"
      - "./.env.docker"
      - "./services/tts_bark/.env.service"
    environment:
      - BARK_USE_SMALL_MODELS=false
    healthcheck:
      test:
        [
          "CMD",
          "python",
          "/app/scripts/health_check.py",
          "http://localhost:7100/health/ready",
          "--timeout",
          "5",
        ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 6G # Increased for Bark
          cpus: "2"
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  testing-ui:
    build:
      context: "."
      dockerfile: "services/testing_ui/Dockerfile"
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/testing-ui:latest"
    ports:
      - "8080:8080" # External: 8080, Internal: 8080
    env_file:
      - "./.env.common"
      - "./.env.docker"
    environment:
      - GRADIO_SERVER_NAME=0.0.0.0
      - GRADIO_SERVER_PORT=8080
      - AUDIO_PREPROCESSOR_URL=http://audio-processor:9100
      - STT_URL=http://stt:9000
      - ORCHESTRATOR_URL=http://orchestrator-enhanced:8200
      - TTS_BARK_URL=http://tts-bark:7100
    depends_on:
      audio-processor:
        condition: service_healthy
      stt:
        condition: service_healthy
      orchestrator-enhanced:
        condition: service_healthy
      tts-bark:
        condition: service_healthy
    healthcheck:
      test:
        [
          "CMD",
          "python",
          "/app/scripts/health_check.py",
          "http://localhost:8081/health/ready",
          "--timeout",
          "5",
        ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: "1"
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  monitoring-dashboard:
    build:
      context: "."
      dockerfile: "services/monitoring_dashboard/Dockerfile"
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/monitoring-dashboard:latest"
    ports:
      - "8501:8501" # External: 8501, Internal: 8501
    env_file:
      - "./.env.common"
      - "./.env.docker"
    environment:
      - STREAMLIT_SERVER_PORT=8501
      - STREAMLIT_SERVER_ADDRESS=0.0.0.0
      - PROMETHEUS_URL=http://prometheus:9090
    depends_on:
      - prometheus
    healthcheck:
      test:
        [
          "CMD",
          "python",
          "/app/scripts/health_check.py",
          "http://localhost:8501/health/ready",
          "--timeout",
          "5",
        ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: "1"
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"
