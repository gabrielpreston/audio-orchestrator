services:
  # Enhanced AI Services
  guardrails:
    build:
      context: "."
      dockerfile: "services/guardrails/Dockerfile"
      cache_from:
        - ghcr.io/gabrielpreston/guardrails:latest
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/guardrails:latest"
    user: "${PUID:-1000}:${PGID:-1000}"
    ports:
      - "9310:9300" # External: 9310, Internal: 9300
    env_file:
      - path: "./.env.common"
        required: false
      - path: "./.env.docker"
        required: false
      - path: "./services/guardrails/.env.service"
        required: false
    environment:
      - TOXICITY_MODEL=unitary/toxic-bert
      - ENABLE_PII_DETECTION=true
      - FORCE_MODEL_DOWNLOAD_TOXICITY_MODEL=${FORCE_MODEL_DOWNLOAD_TOXICITY_MODEL:-false}
      - HF_HOME=/app/models
    volumes:
      - ./services/models/guardrails:/app/models
      # Mount source code for development (overrides COPY in Dockerfile)
      - "./services/common:/app/services/common"
      - "./services/guardrails:/app/services/guardrails"
    deploy:
      resources:
        limits:
          memory: 1G # Increased for better BERT model performance and caching
          cpus: "3" # Increased for faster toxicity inference
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  orchestrator:
    build:
      context: "."
      dockerfile: "services/orchestrator/Dockerfile"
      cache_from:
        - ghcr.io/gabrielpreston/orchestrator:latest
      # cache exported to GHCR services scope
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/orchestrator:latest"
    ports:
      - "8220:8200" # External: 8220, Internal: 8200
    env_file:
      - path: "./.env.common"
        required: false
      - path: "./.env.docker"
        required: false
      - path: "./services/orchestrator/.env.service"
        required: false
    environment:
      - LLM_BASE_URL=http://flan:8100
      - GUARDRAILS_BASE_URL=http://guardrails:9300
    volumes:
      # Mount source code for development (overrides COPY in Dockerfile)
      - "./services/common:/app/services/common"
      - "./services/orchestrator:/app/services/orchestrator"
    deploy:
      resources:
        limits:
          memory: 512M # Orchestrator is lightweight coordinator (reduced from 1G)
          cpus: "3" # Increased from 2 for better concurrent pipeline coordination
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  flan:
    build:
      context: "."
      dockerfile: "services/flan/Dockerfile"
      cache_from:
        - ghcr.io/gabrielpreston/flan:latest
      # duplicate removed
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/flan:latest"
    user: "${PUID:-1000}:${PGID:-1000}"
    ports:
      - "8110:8100" # External: 8110, Internal: 8100
    env_file:
      - path: "./.env.common"
        required: false
      - path: "./.env.docker"
        required: false
      - path: "./services/flan/.env.service"
        required: false
    environment:
      - MODEL_NAME=${FLAN_T5_MODEL_SIZE:-google/flan-t5-large}
      - HF_HOME=/app/models
      - FORCE_MODEL_DOWNLOAD_FLAN_T5=${FORCE_MODEL_DOWNLOAD_FLAN_T5:-false}
      # FLAN optimizations
      - FLAN_ENABLE_TORCH_COMPILE=${FLAN_ENABLE_TORCH_COMPILE:-true}
      - FLAN_COMPILE_MODE=${FLAN_COMPILE_MODE:-default}
      - FLAN_ENABLE_PREWARM=${FLAN_ENABLE_PREWARM:-true}
      - FLAN_ENABLE_CACHE=${FLAN_ENABLE_CACHE:-false}
      - FLAN_CACHE_MAX_ENTRIES=${FLAN_CACHE_MAX_ENTRIES:-100}
      - FLAN_CACHE_MAX_SIZE_MB=${FLAN_CACHE_MAX_SIZE_MB:-500}
    volumes:
      - ./services/models/flan-t5:/app/models
      # Mount source code for development (overrides COPY in Dockerfile)
      - "./services/common:/app/services/common"
      - "./services/flan:/app/services/flan"
    # No dependencies - FLAN-T5 is standalone
    deploy:
      resources:
        limits:
          memory: 7G # FLAN-T5 Large on GPU (optimized - GPU offloads memory)
          cpus: "8" # Increased for maximum GPU utilization
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"
  audio:
    build:
      context: "."
      dockerfile: "services/audio/Dockerfile"
      cache_from:
        - ghcr.io/gabrielpreston/audio:latest
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/audio:latest"
    user: "${PUID:-1000}:${PGID:-1000}"
    ports:
      - "8010:9100"
    env_file:
      - path: "./.env.common"
        required: false
      - path: "./.env.docker"
        required: false
      - path: "./services/audio/.env.service"
        required: false
    environment:
      - FORCE_MODEL_DOWNLOAD_METRICGAN=${FORCE_MODEL_DOWNLOAD_METRICGAN:-false}
      - METRICGAN_MODEL_SAVEDIR=/app/models/metricgan-plus
    volumes:
      - ./services/models/audio:/app/models
      - "./debug:/app/debug"
      - "./services/audio/config:/app/config"
      # Mount source code for development (overrides COPY in Dockerfile)
      - "./services/common:/app/services/common"
      - "./services/audio:/app/services/audio"
    deploy:
      resources:
        limits:
          memory: 2.5G # Audio processing with MetricGAN+ on GPU (+1G for concurrent enhancement)
          cpus: "5" # Increased from 3 for better parallel audio processing and throughput
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  discord:
    build:
      context: "."
      dockerfile: "services/discord/Dockerfile"
      cache_from:
        - ghcr.io/gabrielpreston/discord:latest
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/discord:latest"
    user: "${PUID:-1000}:${PGID:-1000}"
    ports:
      - "8009:8001"
    env_file:
      - path: "./.env.common"
        required: false
      - path: "./.env.docker"
        required: false
      - path: "./services/discord/.env.service"
        required: false
      - path: "./services/discord/.env.secrets"
        required: false
    volumes:
      - "./debug:/app/debug"
      # Mount source code for development (overrides COPY in Dockerfile)
      - "./services/common:/app/services/common"
      - "./services/discord:/app/services/discord"
    deploy:
      resources:
        limits:
          memory: 512M # Discord bot coordination (reduced from 1G)
          cpus: "2" # Increased for voice I/O handling
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  stt:
    build:
      context: "."
      dockerfile: "services/stt/Dockerfile"
      cache_from:
        - ghcr.io/gabrielpreston/stt:latest
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/stt:latest"
    user: "${PUID:-1000}:${PGID:-1000}"
    ports:
      - "8011:9000"
    env_file:
      - path: "./.env.common"
        required: false
      - path: "./.env.docker"
        required: false
      - path: "./services/stt/.env.service"
        required: false
    environment:
      - FORCE_MODEL_DOWNLOAD_WHISPER_MODEL=${FORCE_MODEL_DOWNLOAD_WHISPER_MODEL:-false}
      # STT optimizations (torch.compile() not applicable - faster-whisper uses CTranslate2)
      - STT_ENABLE_PREWARM=${STT_ENABLE_PREWARM:-true}
      - STT_ENABLE_CACHE=${STT_ENABLE_CACHE:-true}
      - STT_CACHE_MAX_ENTRIES=${STT_CACHE_MAX_ENTRIES:-200}
      - STT_CACHE_MAX_SIZE_MB=${STT_CACHE_MAX_SIZE_MB:-1000}
    volumes:
      - "./services/models/stt:/app/models"
      - "./debug:/app/debug"
      # Mount source code for development (overrides COPY in Dockerfile)
      - "./services/common:/app/services/common"
      - "./services/stt:/app/services/stt"
    deploy:
      resources:
        limits:
          memory: 5G # STT with faster-whisper on GPU (+2G for transcript caching and buffers)
          cpus: "8" # Increased from 6 for better GPU pipeline throughput and concurrent requests
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  # Bark TTS Service
  bark:
    build:
      context: "."
      dockerfile: "services/bark/Dockerfile"
      cache_from:
        - ghcr.io/gabrielpreston/bark:latest
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/bark:latest"
    user: "${PUID:-1000}:${PGID:-1000}"
    ports:
      - "7120:7100" # External: 7120, Internal: 7100
    env_file:
      - path: "./.env.common"
        required: false
      - path: "./.env.docker"
        required: false
      - path: "./services/bark/.env.service"
        required: false
    environment:
      - FORCE_MODEL_DOWNLOAD_BARK_MODELS=${FORCE_MODEL_DOWNLOAD_BARK_MODELS:-false}
      - BARK_USE_SMALL_MODELS=true # Enable small models for 30-50% speedup
      - BARK_ENABLE_TORCH_COMPILE=${BARK_ENABLE_TORCH_COMPILE:-true}
      - BARK_COMPILE_MODE=${BARK_COMPILE_MODE:-max-autotune-no-cudagraphs}
      - BARK_ENABLE_PREWARM=${BARK_ENABLE_PREWARM:-true}
      - BARK_ENABLE_CACHE=${BARK_ENABLE_CACHE:-true}
      - BARK_CACHE_MAX_ENTRIES=${BARK_CACHE_MAX_ENTRIES:-100}
      - BARK_CACHE_MAX_SIZE_MB=${BARK_CACHE_MAX_SIZE_MB:-500}
      - BARK_ENABLE_INT8_QUANTIZATION=${BARK_ENABLE_INT8_QUANTIZATION:-false}
      - HF_HOME=/app/models
      - HOME=/app
      - XDG_CACHE_HOME=/app/models
      # PyTorch threading optimization (match CPU count)
      - OMP_NUM_THREADS=8
      - MKL_NUM_THREADS=8
      # CUDA memory optimization (enhances base image setting)
      # Note: docker-compose environment variables override base image settings
      # Base image sets: PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128
      # This adds expandable_segments if supported (verify PyTorch version compatibility)
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True
    volumes:
      - ./services/models/bark:/app/models
      # Mount /app/.cache so Bark can write to ~/.cache/suno/bark_v0
      - ./services/models/bark/.cache:/app/.cache
      # Mount source code for development (overrides COPY in Dockerfile)
      - "./services/common:/app/services/common"
      - "./services/bark:/app/services/bark"
    deploy:
      resources:
        limits:
          memory: 5G # Bark on GPU (optimized for GPU buffer management)
          cpus: "8" # Increased for maximum GPU utilization
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  testing:
    build:
      context: "."
      dockerfile: "services/testing/Dockerfile"
      cache_from:
        - ghcr.io/gabrielpreston/testing:latest
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/testing:latest"
    ports:
      - "8080:8080" # External: 8080, Internal: 8080
    env_file:
      - path: "./.env.common"
        required: false
      - path: "./.env.docker"
        required: false
    environment:
      - GRADIO_SERVER_NAME=0.0.0.0
      - GRADIO_SERVER_PORT=8080
      - AUDIO_BASE_URL=http://audio:9100
      - STT_BASE_URL=http://stt:9000
      - ORCHESTRATOR_BASE_URL=http://orchestrator:8200
      - TTS_BASE_URL=http://bark:7100
    volumes:
      # Mount source code for development (overrides COPY in Dockerfile)
      - "./services/common:/app/services/common"
      - "./services/testing:/app/services/testing"
    deploy:
      resources:
        limits:
          memory: 512M # Testing service (reduced from 1G)
          cpus: "1" # UI service, not CPU-intensive
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  monitoring:
    build:
      context: "."
      dockerfile: "services/monitoring/Dockerfile"
      cache_from:
        - ghcr.io/gabrielpreston/monitoring:latest
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/monitoring:latest"
    ports:
      - "8501:8501" # External: 8501, Internal: 8501
    env_file:
      - path: "./.env.common"
        required: false
      - path: "./.env.docker"
        required: false
    environment:
      - STREAMLIT_SERVER_PORT=8501
      - STREAMLIT_SERVER_ADDRESS=0.0.0.0
      - PROMETHEUS_URL=http://prometheus:9090
    volumes:
      # Mount source code for development (overrides COPY in Dockerfile)
      - "./services/common:/app/services/common"
      - "./services/monitoring:/app/services/monitoring"
    deploy:
      resources:
        limits:
          memory: 512M # Monitoring UI (reduced from 1G)
          cpus: "1" # Streamlit dashboard
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  # Observability Stack - INDEPENDENT INFRASTRUCTURE
  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.91.0
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./config/otel-collector-config.yaml:/etc/otel-collector-config.yaml:ro
    ports:
      - "4317:4317" # OTLP gRPC receiver
      - "4318:4318" # OTLP HTTP receiver
      - "8889:8889" # Prometheus metrics exporter
      - "13133:13133" # Health check
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: "0.5"
    restart: unless-stopped
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  prometheus:
    image: prom/prometheus:v2.48.0
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=7d"
      - "--web.console.libraries=/usr/share/prometheus/console_libraries"
      - "--web.console.templates=/usr/share/prometheus/consoles"
    volumes:
      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    ports:
      - "9090:9090"
    deploy:
      resources:
        limits:
          memory: 512M # Prometheus metrics storage (reduced from 1G)
          cpus: "1" # Metrics aggregation
    restart: unless-stopped
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  jaeger:
    image: jaegertracing/all-in-one:1.52
    environment:
      - COLLECTOR_OTLP_ENABLED=true
      - SPAN_STORAGE_TYPE=memory
      - LOG_LEVEL=warn
    ports:
      - "16686:16686" # Jaeger UI
      - "14250:14250" # gRPC
      - "14268:14268" # HTTP
    deploy:
      resources:
        limits:
          memory: 512M # Jaeger tracing (reduced from 1G)
          cpus: "1" # Trace storage and query
    restart: unless-stopped
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  grafana:
    image: grafana/grafana:10.2.2
    env_file:
      - path: "./.env.common"
        required: false
    entrypoint: ["/bin/sh", "/etc/grafana/entrypoint.sh"]
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_PATHS_PROVISIONING=/etc/grafana/provisioning
      - GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-simple-json-datasource
      # Use project-standard logging configuration from .env.common
      # LOG_LEVEL defaults to "info", LOG_JSON defaults to "true" in .env.common
      # Entrypoint script maps LOG_JSON to GF_LOG_FORMAT: true/True/1 → json, otherwise → console
      - GF_LOG_LEVEL=${LOG_LEVEL:-info}
      # LOG_JSON is read from .env.common by env_file, entrypoint script converts it to GF_LOG_FORMAT
      # To override format: set GF_LOG_FORMAT_OVERRIDE=json or GF_LOG_FORMAT_OVERRIDE=console
    volumes:
      - ./config/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./config/grafana/entrypoint.sh:/etc/grafana/entrypoint.sh:ro
      - grafana-data:/var/lib/grafana
    ports:
      - "3000:3000"
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: "0.5"
    restart: unless-stopped
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

volumes:
  prometheus-data: {}
  grafana-data: {}
  # Shared pip cache for faster builds across services
  pip-cache:
    driver: local
