services:
  # Enhanced AI Services
  guardrails:
    build:
      context: "."
      dockerfile: "services/guardrails/Dockerfile"
      cache_from:
        - ghcr.io/gabrielpreston/guardrails:latest
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/guardrails:latest"
    user: "${PUID:-1000}:${PGID:-1000}"
    ports:
      - "9310:9300" # External: 9310, Internal: 9300
    env_file:
      - path: "./.env.common"
        required: false
      - path: "./.env.docker"
        required: false
      - path: "./services/guardrails/.env.service"
        required: false
    # Configuration moved to services/guardrails/.env.service
    # See: TOXICITY_MODEL, ENABLE_PII_DETECTION, FORCE_MODEL_DOWNLOAD_TOXICITY_MODEL
    volumes:
      - ./services/models/guardrails:/app/models
      # Mount source code for development (overrides COPY in Dockerfile)
      - "./services/common:/app/services/common"
      - "./services/guardrails:/app/services/guardrails"
    deploy:
      resources:
        limits:
          memory: 1.5G # Increased for better BERT model performance and caching
          cpus: "3.5" # Increased for faster toxicity inference
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  orchestrator:
    build:
      context: "."
      dockerfile: "services/orchestrator/Dockerfile"
      cache_from:
        - ghcr.io/gabrielpreston/orchestrator:latest
      # cache exported to GHCR services scope
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/orchestrator:latest"
    ports:
      - "8220:8200" # External: 8220, Internal: 8200
    env_file:
      - path: "./.env.common"
        required: false
      - path: "./.env.docker"
        required: false
      - path: "./services/orchestrator/.env.service"
        required: false
    volumes:
      # Mount source code for development (overrides COPY in Dockerfile)
      - "./services/common:/app/services/common"
      - "./services/orchestrator:/app/services/orchestrator"
    deploy:
      resources:
        limits:
          memory: 1G # Increased for better concurrent request handling
          cpus: "4" # Increased for better concurrent pipeline coordination
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  flan:
    build:
      context: "."
      dockerfile: "services/flan/Dockerfile"
      cache_from:
        - ghcr.io/gabrielpreston/flan:latest
      # duplicate removed
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/flan:latest"
    user: "${PUID:-1000}:${PGID:-1000}"
    ports:
      - "8110:8100" # External: 8110, Internal: 8100
    env_file:
      - path: "./.env.common"
        required: false
      - path: "./.env.docker"
        required: false
      - path: "./services/flan/.env.service"
        required: false
    # Configuration moved to services/flan/.env.service
    # See: HF_HOME, FORCE_MODEL_DOWNLOAD_FLAN_T5, FLAN_* optimization flags
    # Note: MODEL_NAME removed - service derives it from FLAN_T5_MODEL_SIZE
    volumes:
      - ./services/models/flan-t5:/app/models
      # Mount source code for development (overrides COPY in Dockerfile)
      - "./services/common:/app/services/common"
      - "./services/flan:/app/services/flan"
    # No dependencies - FLAN-T5 is standalone
    deploy:
      resources:
        limits:
          memory: 8G # FLAN-T5 Large on GPU (optimized - GPU offloads memory)
          cpus: "9" # Increased for maximum GPU utilization
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"
  discord:
    build:
      context: "."
      dockerfile: "services/discord/Dockerfile"
      cache_from:
        - ghcr.io/gabrielpreston/discord:latest
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/discord:latest"
    user: "${PUID:-1000}:${PGID:-1000}"
    ports:
      - "8009:8001"
    env_file:
      - path: "./.env.common"
        required: false
      - path: "./.env.docker"
        required: false
      - path: "./services/discord/.env.service"
        required: false
      - path: "./services/discord/.env.secrets"
        required: false
    environment:
      - HOME=/app # Ensure ~ resolves to /app for openwakeword default download location
    volumes:
      - "./debug:/app/debug"
      - "./services/models/wake/detection:/app/.local/share/openwakeword/models" # Persist wake detection models
      - "./services/models/wake/infrastructure:/usr/local/lib/python3.11/site-packages/openwakeword/resources/models" # Persist infrastructure models
      # Mount source code for development (overrides COPY in Dockerfile)
      - "./services/common:/app/services/common"
      - "./services/discord:/app/services/discord"
    deploy:
      resources:
        limits:
          memory: 768M # Increased for voice processing buffers
          cpus: "2.5" # Increased for better I/O handling
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  stt:
    build:
      context: "."
      dockerfile: "services/stt/Dockerfile"
      cache_from:
        - ghcr.io/gabrielpreston/stt:latest
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/stt:latest"
    user: "${PUID:-1000}:${PGID:-1000}"
    ports:
      - "8011:9000"
    env_file:
      - path: "./.env.common"
        required: false
      - path: "./.env.docker"
        required: false
      - path: "./services/stt/.env.service"
        required: false
    # Configuration moved to services/stt/.env.service
    # See: FORCE_MODEL_DOWNLOAD_WHISPER_MODEL, STT_* optimization flags
    volumes:
      - "./services/models/stt:/app/models"
      - "./debug:/app/debug"
      # Mount source code for development (overrides COPY in Dockerfile)
      - "./services/common:/app/services/common"
      - "./services/stt:/app/services/stt"
    deploy:
      resources:
        limits:
          memory: 6G # STT with faster-whisper on GPU
          cpus: "9" # Increased for better GPU pipeline throughput
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  # Bark TTS Service
  bark:
    build:
      context: "."
      dockerfile: "services/bark/Dockerfile"
      cache_from:
        - ghcr.io/gabrielpreston/bark:latest
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/bark:latest"
    user: "${PUID:-1000}:${PGID:-1000}"
    ports:
      - "7120:7100" # External: 7120, Internal: 7100
    env_file:
      - path: "./.env.common"
        required: false
      - path: "./.env.docker"
        required: false
      - path: "./services/bark/.env.service"
        required: false
    # Configuration moved to services/bark/.env.service
    # See: FORCE_MODEL_DOWNLOAD_BARK_MODELS, BARK_* flags, PyTorch optimizations
    volumes:
      - ./services/models/bark:/app/models
      # Mount /app/.cache so Bark can write to ~/.cache/suno/bark_v0
      - ./services/models/bark/.cache:/app/.cache
      # Mount source code for development (overrides COPY in Dockerfile)
      - "./services/common:/app/services/common"
      - "./services/bark:/app/services/bark"
    deploy:
      resources:
        limits:
          memory: 6G # Bark on GPU (optimized for GPU buffer management)
          cpus: "9" # Increased for maximum GPU utilization
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  testing:
    build:
      context: "."
      dockerfile: "services/testing/Dockerfile"
      cache_from:
        - ghcr.io/gabrielpreston/testing:latest
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/testing:latest"
    ports:
      - "8080:8080" # External: 8080, Internal: 8080
    env_file:
      - path: "./.env.common"
        required: false
      - path: "./.env.docker"
        required: false
    environment:
      - GRADIO_SERVER_NAME=0.0.0.0
      - GRADIO_SERVER_PORT=8080
    volumes:
      # Mount source code for development (overrides COPY in Dockerfile)
      - "./services/common:/app/services/common"
      - "./services/testing:/app/services/testing"
      # Mount WAV file storage
      - "./test_artifacts/testing/wavs:/app/wavs"
    deploy:
      resources:
        limits:
          memory: 512M # Testing service (reduced from 1G)
          cpus: "1" # UI service, not CPU-intensive
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

volumes:
  # Shared pip cache for faster builds across services
  pip-cache:
    driver: local
