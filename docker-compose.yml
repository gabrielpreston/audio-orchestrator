version: "3.8"
services:
  # Enhanced AI Services
  guardrails:
    build:
      context: "."
      dockerfile: "services/guardrails/Dockerfile"
      cache_from:
        - type=gha,scope=services
        - ghcr.io/gabrielpreston/python-ml:latest
        - ghcr.io/gabrielpreston/guardrails:latest
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/guardrails:latest"
    ports:
      - "9310:9300" # External: 9310, Internal: 9300
    env_file:
      - "./.env.common"
      - "./.env.docker"
      - "./services/guardrails/.env.service"
    environment:
      - TOXICITY_MODEL=unitary/toxic-bert
      - ENABLE_PII_DETECTION=true
    healthcheck:
      test:
        [
          "CMD",
          "python",
          "/app/scripts/health_check.py",
          "http://localhost:9300/health/ready",
          "--timeout",
          "5",
        ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 1G # Toxicity detection (optimized)
          cpus: "1"
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  orchestrator-enhanced:
    build:
      context: "."
      dockerfile: "services/orchestrator_enhanced/Dockerfile"
      cache_from:
        - type=gha,scope=services
        - ghcr.io/gabrielpreston/python-ml:latest
        - ghcr.io/gabrielpreston/orchestrator_enhanced:latest
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/orchestrator-enhanced:latest"
    ports:
      - "8220:8200" # External: 8220, Internal: 8200
    env_file:
      - "./.env.common"
      - "./.env.docker"
      - "./services/orchestrator_enhanced/.env.service"
    environment:
      - LLM_PRIMARY_URL=http://llm-flan:8100
      - GUARDRAILS_URL=http://guardrails:9300
    depends_on:
      llm-flan:
        condition: service_healthy
      guardrails:
        condition: service_healthy
      audio-processor:
        condition: service_healthy
      stt:
        condition: service_healthy
      tts-bark:
        condition: service_healthy
    healthcheck:
      test:
        [
          "CMD",
          "python",
          "/app/scripts/health_check.py",
          "http://localhost:8200/health/ready",
          "--timeout",
          "5",
        ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 1G # LangChain + overhead (optimized)
          cpus: "2"
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  llm-flan:
    build:
      context: "."
      dockerfile: "services/llm_flan/Dockerfile"
      cache_from:
        - type=gha,scope=services
        - ghcr.io/gabrielpreston/python-ml:latest
        - ghcr.io/gabrielpreston/llm_flan:latest
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/llm-flan:latest"
    ports:
      - "8110:8100" # External: 8110, Internal: 8100
    env_file:
      - "./.env.common"
      - "./.env.docker"
      - "./services/llm_flan/.env.service"
    environment:
      - MODEL_NAME=${FLAN_T5_MODEL_SIZE:-google/flan-t5-large}
      - TRANSFORMERS_CACHE=/app/models
    volumes:
      - ./services/models/flan-t5:/app/models:ro
      - ~/.cache/huggingface:/root/.cache/huggingface:ro
    # No dependencies - FLAN-T5 is standalone
    healthcheck:
      test:
        [
          "CMD",
          "python",
          "/app/scripts/health_check.py",
          "http://localhost:8100/health/ready",
          "--timeout",
          "10",
        ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 300s
    deploy:
      resources:
        limits:
          memory: 10G # Optimized for FLAN-T5 Large + overhead
          cpus: "4"
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"
  audio-processor:
    build:
      context: "."
      dockerfile: "services/audio_processor/Dockerfile"
      cache_from:
        - type=gha,scope=services
        - ghcr.io/gabrielpreston/python-ml:latest
        - ghcr.io/gabrielpreston/audio_processor:latest
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/audio-processor:latest"
    ports:
      - "8010:9100"
    env_file:
      - "./.env.common"
      - "./.env.docker"
      - "./services/audio_processor/.env.service"
    volumes:
      - "./debug:/app/debug"
      - "./services/audio_processor/config:/app/config"
    healthcheck:
      test:
        [
          "CMD",
          "python",
          "/app/scripts/health_check.py",
          "http://localhost:9100/health/ready",
          "--timeout",
          "5",
        ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 1G # Audio processing
          cpus: "1"
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  discord:
    build:
      context: "."
      dockerfile: "services/discord/Dockerfile"
      cache_from:
        - type=gha,scope=services
        - ghcr.io/gabrielpreston/python-audio:latest
        - ghcr.io/gabrielpreston/discord:latest
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/discord:latest"
    ports:
      - "8009:8001"
    env_file:
      - "./.env.common"
      - "./.env.docker"
      - "./services/discord/.env.service"
    volumes:
      - "./debug:/app/debug"
    healthcheck:
      test: >
        ["CMD", "python", "/app/scripts/health_check.py",
         "http://localhost:8001/health/ready", "--timeout", "5"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 45s
    depends_on:
      audio-processor:
        condition: service_healthy
      stt:
        condition: service_healthy
      orchestrator-enhanced:
        condition: service_healthy
    deploy:
      resources:
        limits:
          memory: 1G # Discord bot + voice processing
          cpus: "1"
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  stt:
    build:
      context: "."
      dockerfile: "services/stt/Dockerfile"
      cache_from:
        - type=gha,scope=services
        - ghcr.io/gabrielpreston/python-ml:latest
        - ghcr.io/gabrielpreston/stt:latest
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/stt:latest"
    ports:
      - "8011:9000"
    env_file:
      - "./.env.common"
      - "./.env.docker"
      - "./services/stt/.env.service"
    volumes:
      - "./services/models/stt:/app/models:ro"
      - "./debug:/app/debug"
    healthcheck:
      test: >
        ["CMD", "python", "/app/scripts/health_check.py",
         "http://localhost:9000/health/ready", "--timeout", "5"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    depends_on:
      audio-processor:
        condition: service_healthy
    deploy:
      resources:
        limits:
          memory: 2G # STT with faster-whisper models
          cpus: "2"
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  # Bark TTS Service
  tts-bark:
    build:
      context: "."
      dockerfile: "services/tts_bark/Dockerfile"
      cache_from:
        - type=gha,scope=services
        - ghcr.io/gabrielpreston/python-ml:latest
        - ghcr.io/gabrielpreston/tts_bark:latest
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/tts-bark:latest"
    ports:
      - "7120:7100" # External: 7120, Internal: 7100
    env_file:
      - "./.env.common"
      - "./.env.docker"
      - "./services/tts_bark/.env.service"
    environment:
      - BARK_USE_SMALL_MODELS=false
    healthcheck:
      test:
        [
          "CMD",
          "python",
          "/app/scripts/health_check.py",
          "http://localhost:7100/health/ready",
          "--timeout",
          "5",
        ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 4G # Optimized for Bark
          cpus: "2"
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  testing-ui:
    build:
      context: "."
      dockerfile: "services/testing_ui/Dockerfile"
      cache_from:
        - type=gha,scope=services
        - ghcr.io/gabrielpreston/python-ml:latest
        - ghcr.io/gabrielpreston/testing-ui:latest
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/testing-ui:latest"
    ports:
      - "8080:8080" # External: 8080, Internal: 8080
    env_file:
      - "./.env.common"
      - "./.env.docker"
    environment:
      - GRADIO_SERVER_NAME=0.0.0.0
      - GRADIO_SERVER_PORT=8080
      - AUDIO_PREPROCESSOR_URL=http://audio-processor:9100
      - STT_URL=http://stt:9000
      - ORCHESTRATOR_URL=http://orchestrator-enhanced:8200
      - TTS_BARK_URL=http://tts-bark:7100
    depends_on:
      audio-processor:
        condition: service_healthy
      stt:
        condition: service_healthy
      orchestrator-enhanced:
        condition: service_healthy
      tts-bark:
        condition: service_healthy
    healthcheck:
      test:
        [
          "CMD",
          "python",
          "/app/scripts/health_check.py",
          "http://localhost:8080/health/ready",
          "--timeout",
          "5",
        ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: "1"
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  monitoring-dashboard:
    build:
      context: "."
      dockerfile: "services/monitoring_dashboard/Dockerfile"
      cache_from:
        - type=gha,scope=services
        - ghcr.io/gabrielpreston/python-ml:latest
        - ghcr.io/gabrielpreston/monitoring-dashboard:latest
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/monitoring-dashboard:latest"
    ports:
      - "8501:8501" # External: 8501, Internal: 8501
    env_file:
      - "./.env.common"
      - "./.env.docker"
    environment:
      - STREAMLIT_SERVER_PORT=8501
      - STREAMLIT_SERVER_ADDRESS=0.0.0.0
    healthcheck:
      test:
        [
          "CMD",
          "python",
          "/app/scripts/health_check.py",
          "http://localhost:8501/health/ready",
          "--timeout",
          "5",
        ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: "1"
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  # Observability Stack - INDEPENDENT INFRASTRUCTURE
  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.91.0
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./config/otel-collector-config.yaml:/etc/otel-collector-config.yaml:ro
    ports:
      - "4317:4317" # OTLP gRPC receiver
      - "4318:4318" # OTLP HTTP receiver
      - "8889:8889" # Prometheus metrics exporter
      - "13133:13133" # Health check
    healthcheck:
      test:
        [
          "CMD",
          "python",
          "/app/scripts/health_check.py",
          "http://localhost:13133/",
          "--timeout",
          "5",
        ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: "0.5"
    restart: unless-stopped
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  prometheus:
    image: prom/prometheus:v2.48.0
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=7d"
      - "--web.console.libraries=/usr/share/prometheus/console_libraries"
      - "--web.console.templates=/usr/share/prometheus/consoles"
    volumes:
      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    ports:
      - "9090:9090"
    healthcheck:
      test:
        [
          "CMD",
          "python",
          "/app/scripts/health_check.py",
          "http://localhost:9090/-/healthy",
          "--timeout",
          "5",
        ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: "1"
    restart: unless-stopped
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  jaeger:
    image: jaegertracing/all-in-one:1.52
    environment:
      - COLLECTOR_OTLP_ENABLED=true
      - SPAN_STORAGE_TYPE=memory
    ports:
      - "16686:16686" # Jaeger UI
      - "14250:14250" # gRPC
      - "14268:14268" # HTTP
    healthcheck:
      test:
        [
          "CMD",
          "python",
          "/app/scripts/health_check.py",
          "http://localhost:16686/",
          "--timeout",
          "5",
        ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: "1"
    restart: unless-stopped
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  grafana:
    image: grafana/grafana:10.2.2
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_PATHS_PROVISIONING=/etc/grafana/provisioning
    volumes:
      - ./config/grafana/datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml:ro
      - ./config/grafana/dashboards.yml:/etc/grafana/provisioning/dashboards/dashboards.yml:ro
      - ./config/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - grafana-data:/var/lib/grafana
    ports:
      - "3000:3000"
    healthcheck:
      test:
        [
          "CMD",
          "python",
          "/app/scripts/health_check.py",
          "http://localhost:3000/api/health",
          "--timeout",
          "5",
        ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: "0.5"
    restart: unless-stopped
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

volumes:
  prometheus-data: {}
  grafana-data: {}
  # Shared pip cache for faster builds across services
  pip-cache:
    driver: local
