version: "3.8"
services:
  # Enhanced AI Services
  llm-flan:
    build:
      context: "."
      dockerfile: "services/llm_flan/Dockerfile"
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/llm-flan:latest"
    ports:
      - "8110:8100" # External: 8110, Internal: 8100
    env_file:
      - "./.env.common"
      - "./.env.docker"
      - "./services/llm_flan/.env.service"
    environment:
      - MODEL_NAME=${FLAN_T5_MODEL_SIZE:-google/flan-t5-large}
      - TRANSFORMERS_CACHE=/app/models
    volumes:
      - ./services/models/flan-t5:/app/models:ro
      - ~/.cache/huggingface:/root/.cache/huggingface:ro
    # No dependencies - FLAN-T5 is standalone
    healthcheck:
      test:
        [
          "CMD",
          "python",
          "/app/scripts/health_check.py",
          "http://localhost:8100/health/ready",
          "--timeout",
          "10",
        ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 300s
    deploy:
      resources:
        limits:
          memory: 12G # Increased for FLAN-T5 Large + overhead
          cpus: "4"
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"
  audio-processor:
    build:
      context: "."
      dockerfile: "services/audio_processor/Dockerfile"
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/audio-processor:latest"
    ports:
      - "8010:9100"
    env_file:
      - "./.env.common"
      - "./.env.docker"
      - "./services/audio_processor/.env.service"
    volumes:
      - "./debug:/app/debug"
      - "./services/audio_processor/config:/app/config"
    healthcheck:
      test: >
        ["CMD", "curl", "-f", "http://localhost:9100/health/ready"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  discord:
    build:
      context: "."
      dockerfile: "services/discord/Dockerfile"
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/discord:latest"
    ports:
      - "8009:8001"
    env_file:
      - "./.env.common"
      - "./.env.docker"
      - "./services/discord/.env.service"
    volumes:
      - "./debug:/app/debug"
    healthcheck:
      test: >
        ["CMD", "python", "/app/scripts/health_check.py",
         "http://localhost:8001/health/ready", "--timeout", "5"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 45s
    depends_on:
      audio-processor:
        condition: service_healthy
      stt:
        condition: service_healthy
      orchestrator:
        condition: service_healthy
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  stt:
    build:
      context: "."
      dockerfile: "services/stt/Dockerfile"
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/stt:latest"
    ports:
      - "8011:9000"
    env_file:
      - "./.env.common"
      - "./.env.docker"
      - "./services/stt/.env.service"
    volumes:
      - "./services/models/stt:/app/models:ro"
      - "./debug:/app/debug"
    healthcheck:
      test: >
        ["CMD", "python", "/app/scripts/health_check.py",
         "http://localhost:9000/health/ready", "--timeout", "5"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    depends_on:
      audio-processor:
        condition: service_healthy
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  llm:
    build:
      context: "."
      dockerfile: "services/llm/Dockerfile"
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/llm:latest"
    env_file:
      - "./.env.common"
      - "./.env.docker"
      - "./services/llm/.env.service"
    ports:
      - "8017:8000"
    volumes:
      - "./services/models/llm:/app/models:ro"
      - "./debug:/app/debug"
    healthcheck:
      test: >
        ["CMD", "python", "/app/scripts/health_check.py",
         "http://localhost:8000/health/ready", "--timeout", "5"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  orchestrator:
    build:
      context: "."
      dockerfile: "services/orchestrator/Dockerfile"
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/orchestrator:latest"
    env_file:
      - "./.env.common"
      - "./.env.docker"
      - "./services/orchestrator/.env.service"
    ports:
      - "8023:8000"
    volumes:
      - "./mcp.json:/app/mcp.json:ro"
      - "./services/discord:/app/services/discord:ro"
      - "./debug:/app/debug"
    healthcheck:
      test: >
        ["CMD", "python", "/app/scripts/health_check.py",
         "http://localhost:8000/health/ready", "--timeout", "5"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    depends_on:
      llm:
        condition: service_healthy
      tts:
        condition: service_healthy
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  tts:
    build:
      context: "."
      dockerfile: "services/tts/Dockerfile"
      args:
        BUILDKIT_INLINE_CACHE: "1"
    image: "ghcr.io/gabrielpreston/tts:latest"
    env_file:
      - "./.env.common"
      - "./.env.docker"
      - "./services/tts/.env.service"
    ports:
      - "8027:7000"
    volumes:
      - "./services/models/tts:/app/models:ro"
      - "./debug:/app/debug"
    healthcheck:
      test: >
        ["CMD", "python", "/app/scripts/health_check.py",
         "http://localhost:7000/health/ready", "--timeout", "5"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    restart: "unless-stopped"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"
